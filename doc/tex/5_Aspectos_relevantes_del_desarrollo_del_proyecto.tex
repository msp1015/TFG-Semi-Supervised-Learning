\capitulo{5}{Aspectos relevantes del desarrollo del proyecto}
En la sección que sigue, se abordan los aspectos más significativos que han marcado el desarrollo de este proyecto. Se detalla cómo cada elección ha influido en la trayectoria y los resultados del proyecto.
\section{Lectura artículos científicos}
Se ha decidido incluir la lectura de artículos científicos en aspectos relevantes ya que son una parte significativa de todo el trabajo, llevando bastante tiempo en aquellos que son más técnicos. Además proporcionan una base sólida de teorías y métodos que pueden ser utilizadas tanto para el trabajo como para otras situaciones. Todos ellos están en inglés por lo que también se mejora esta habilidad, notando cierta mejora de comprensión en los últimos artículos.
Un artículo científico es un informe escrito y publicado en una revista con cierto prestigio que describe resultados originales de investigación. Estos artículos son fundamentales para la difusión del conocimiento científico y suelen seguir un formato estructurado que incluye una introducción, metodología, resultados, discusión y conclusiones.

Para este trabajo se han leido dos tipos de artículos, los \textit{surveys} o resúmenes sobre un tema y los artículos de implementación de algoritmos:
\begin{itemize}
	\item \textbf{\textit{A Survey on Semi-Supervised Learning}}~\cite{Engelen:semi-supervised}: Este artículo proporciona una revisión exhaustiva del campo del aprendizaje semi-supervisado. Gracias a este artículo se consigue una base sólida en este tipo de aprendizaje y se comprenden cuales son los desafios actuales y las direcciones futuras en la investigación en este campo.
	\item \textbf{\textit{Ensemble Based Systems in Decision Making}}~\cite{ensembles}: Este artículo revisa el uso de sistemas basados en \textit{ensembles} para la toma de decisiones. Estos sistemas combinan múltiples clasificadores para mejorar la precisión y la robustez del proceso de decisión. Destaca cómo los sistemas de ensamblaje pueden superar las limitaciones de los clasificadores individuales y proporcionar decisiones más confiables y precisas.
	\item \textbf{\textit{Improve Computer-Aided Diagnosis With Machine Learning Techniques Using Undiagnosed Samples}}~\cite{IEEE:CoForest}: Los autores presentan el método Co-Forest, una técnica semi-supervisada que combina múltiples clasificadores de árboles de decisión para mejorar la precisión del diagnóstico. Se discuten los beneficios de incorporar datos no etiquetados en el proceso de entrenamiento y se presentan resultados experimentales que demuestran la eficacia de esta metodología en varias aplicaciones médicas.
	\item \textbf{\textit{Graph-based Semi-supervised Learning: A Comprehensive Review}}~\cite{GSSL:review}: Proporciona una revisión exhaustiva de los métodos de aprendizaje semisupervisado basados en grafos (GSSL). Sus principales contribuciones son: nueva taxonomía, revisión completa de los algoritmos en las diferentes fases y direcciones futuras.
	\item \textbf{\textit{Graph Construction Based on Labeled Instances for Semi-supervised Learning}}~\cite{gbili}: Los autores presentan un método para la construcción de grafos basado en instancias etiquetadas para el aprendizaje semisupervisado. La idea principal es utilizar la información de las instancias etiquetadas para guiar la construcción del grafo.
	\item \textbf{\textit{Learning with Local and Global Consistency}}~\cite{LGC}: Este artículo introduce un algoritmo para el aprendizaje con consistencia local y global. Los autores describen cómo el algoritmo itera para ajustar las etiquetas de las instancias no etiquetadas, combinando la información de las instancias vecinas y la información inicial. Se demuestra que este enfoque es eficaz para tareas de clasificación semi-supervisada.
	\item \textbf{\textit{RGCLI: Robust Graph that Considers Labeled Instances for Semi-Supervised Learning}}~\cite{rgcli}: Presenta una mejora del método GBILI para la construcción de grafos. Los autores cuentan como RGCLI reduce la complejidad, permitiendo manejar conjuntos de datos grandes. Mejora la topología del grafo usando instancias etiquetadas, optimizando la propagación de etiquetas y logrando mayor precisión en la clasificación.
\end{itemize}
\section{Trabajo Preexistente}
La elección de este trabajo se realiza por el gran interés en la inteligencia artificial y el aprendizaje automático, pero también por el hecho de que ya existía un trabajo realizado por otro alumno un año atrás (David Martínez Acha -- \url{https://vass.dmacha.dev/}). Esto ayudaría mucho en el desarrollo ya que serviría como referencia para muchas dudas.

El plan original era hacer mi propia página web educativa desde cero, pero mostrando otra serie de algoritmos semisupervisados (ensembles y grafos) en lugar de los ya existentes. Con el desarrollo del primer algoritmo surge la idea por parte del tutor de basar el proyecto en esta otra página desarrollada por David Martínez. De esta manera, el tiempo que hubiera empleado en aprender e implementar la web desde cero, se emplea en comprender todo el código programado por David, aprovechando también la gestión de cuentas de usuarios. Aún así, se deja total libertad para cambiar e implementar lo que haga falta para mejorar el proyecto original.

Inicialmente se piensa que con un fork a su repositorio de GitHub,~\cite{GH:VASS}, se puede trabajar mejor, pero de esta manera se perderían las tareas y commits hechos hasta la fecha en el repositorio de este proyecto. Por esto se decide descargar el contenido y copiarlo al proyecto ya en desarrollo.

La documentación de David~\cite{TFG:David}, sirve de gran ayuda y también se heredan partes de ella, como los trabajos relacionados, requisitos o técnicas. 

\subsubsection{Cambios en la web}
Gran parte de la web se reutiliza, pero a medida que se desarrolla el proyecto surgen nuevas ideas que modifican parte del comportamiento de la web que ya existía. Esta sección servirá para remarcar esas pequeñas o grandes modificaciones que sufre el diseño inicial y cual es la idea de esta nueva funcionalidad. 
La primera idea de cambio que surge es la de modificar la funcionalidad de configuración de un algoritmo. Visualizando páginas web, se da con una página que muestra algoritmos de aprendizaje automático~ \cite{web:ml-visualizer}, pero no de la misma manera que David. En esta página se permite configurar y ver resultados y estadísticas en una única ventana, con la gran diferencia de que no muetra el paso a paso en cada algoritmo, sino directamente el resultado final de la clasificación.

Posteriormente, gracias al tutor nos damos cuenta de que si la idea es mostrar la ejecución paso a paso, que los parámetros de configuración estén en la misma pantalla, no es de gran ayuda. Por esto, se decide volver a la versión del trabajo base y modificar las plantillas necesarias para adaptarlas a los nuevos algoritmos.

\textbf{Configuración de parámetros:} Cuando tratamos con archivos de datos preparados para estos algoritmos, siempre se suele dar el caso de que la clase o etiqueta suele ser la última columna. Por esto, se ha mantenido la opción de seleccionar el atributo deseado, pero saldrá por defecto la clase en la caja de entrada. Esto además permite mayor agilidad a la hora de hacer pruebas de visualización ya que no hay que emplear tiempo en seleccionar esta opción.

\textbf{Gráfica de estadísticas específicas:} el cambio que se ha realizado en esta parte ha sido la opción de marcar o desmarcar todos los clasificadores para ver su traza en la gráfica de estadísticas. Sirve sobretodo en el caso del Co-Forest por el hecho de que puede haber gran cantidad de árboles, y querer ver uno concreto con la configuración anterior llevaba una perdida de tiempo innecesaria desmarcando uno por uno el resto de clasificadores.

\textbf{Utilizar fichero por defecto}: en la versión anterior se dedica una ventana entera a la selección del archivo, permitiendo descargar varios ficheros de prueba para luego subirlos. La idea aquí es agilizar el proceso de utilizar estos ficheros de prueba, permitiendo que al pulsar en un dataset de prueba se cargue en la sesión directamente, sin tener que actualizarlo desde las descargas. Además, se muestra el contenido de este archivo en forma de tabla.

\textbf{Tarjetas de selección iniciales}: se da un nuevo estilo a las tarjetas iniciales de selección de algoritmos, orientándolas de manera horizontal y añadiendo algo más de contexto para el usuario primerizo. Además, se añade la opción de encontrar el artículo en el que se ha basado la implementación.
\section{Algoritmos}
En esta sección se comentarán los aspectos más relevantes en la implementación de los algoritmos semisupervisados.
\subsection{Co-Forest}\label{sec5:coforest}
En la implementación de este algoritmo de nuevo se parte con una gran ventaja. El año anterior otra alumna había implementado el mismo algoritmo para otro proyecto. Dentro de esta aplicación, Patricia Hernando, hizo sus propios estudios, los cuales se han aprovechado en este trabajo.
Basado en el pseudocodigo del artículo \cite{IEEE:CoForest}, la implementación se ve alterada por el parámetro $W$, el cual establece la confianza en las muestras para ser seleccionada o no. Resumiendo el estudio de Patricia, como se puede ver en el pseudocodigo del articulo, hay una ecuación donde el valor de W esta dividiendo. Esto es un problema ya que según establece el algoritmo puede llegar a valer cero, provocando así una indeterminación. Uno de los estudios de Patricia determina que una de las mejores soluciones a esto es iniciar el parámetro W al minimo entre 100 y el 10\% de la cantidad de muestras etiquetadas que hay. Como se puede ver en el pseudocodigo de la sección tres, esto se aprovecha, evitando así posibles problemas.
Para determinar si el algoritmo definitivo es bueno, se compara con el de Patricia, evaluando como varía el valor de \textit{accuracy} en cada iteración del algoritmo. Los resultados se muestran en la figura \ref{fig:../img/memoria/ComparacionCoForest.png}.
\imagen{../img/memoria/ComparacionCoForest.png}{Comparación algoritmo CoForest utilizando diversos datasets}{0.75}

\subsection{Grafos}
En este apartado se comentarán todos los aspectos relevantes relacionados con la implementación de los algoritmos basados en grafos. Desde sus posibles interpretaciones y modificaciones con respecto al código original, a los diferentes estudios de comparación.

\subsubsection{Comparativa de bibliotecas de grafos}
Cuando se quiere implementar un algoritmo basado en grafos, lo ideal es utilizar una biblioteca que ayude a automatizar y mejorar el código. En \textit{Python}, existen varias bibliotecas que ayudan en esta tarea, tres de ellas son: \textit{\textbf{NetworkX}}, \textit{\textbf{igraph}} y \textit{\textbf{graph-tool}}. En esta sección se resumirá el estudio realizado para elegir la opción que mejor se adapte a las especificaciones.

Todas ellas ayudan en la construcción de grafos, pero para empezar es necesario dejar claro para qué se va a utilizar esta biblioteca. En cuanto al tamaño de los grafos, no necesariamente se necesita algo que maneje grafos muy grandes (más de 10000 nodos) de manera efectiva. La mayoría de \textit{datasets} utilizados tendrán muchas menos instancias. En cuanto a la velocidad, se busca algo que sea efectivo pero sin necesidad de buscar lo mejor o más rápido, ya que los datos de entrada no van a suponer un gran esfuerzo. También hay que tener en cuenta la integración que se llevará a cabo posteriormente en la web, posiblemente con herramientas como \textit{d3.js}.

Tras una primera búsqueda queda claro que la herramienta de \textit{graph-tool} tiene un objetivo mucho más amplio y está pensado para proyectos con grafos grandes. De hecho es una herramienta que no se instala con \textit{pip} sino que necesita otra instalación.

Por lo tanto, descartada una opción, se realizará una pequeña prueba para llegar a una conclusión. A continuación se muestra el pseudocódigo utilizado para la comparación de herramientas.

\begin{algorithm}
	\label{testGraph}
	\KwIn{Dataset de prueba $L$ \textit{(digits)}}
	\KwOut{Grafo $G$ en formato \textit{JSON}}
	\BlankLine
	\textit{timer} $\leftarrow$ \textit{startTimer}()\\
	$G \leftarrow \emptyset$\\
	$D \leftarrow$ \textit{pairwiseDistances}($L$) // Matriz de distancias\\
	\For{$i = 0$ \KwTo $|L|-1$}{
		// Agregar vértices al grafo para cada muestra de $L$\\
		$G \leftarrow$ \textit{addNode}($i$)\\
	}
	$k \leftarrow 5$\\
	\For{$i = 0$ \KwTo $|L|-1$}{
		$kNN \leftarrow$ \textit{getKNearestNeighbors}($D[i]$, $k$)\\
		\For{$j \in kNN$}{
			// Añadir arista entre $i$ y $j$ con el peso de la distancia\\
			$G \leftarrow$ \textit{addEdge}($i$, $j$, $D[i][j]$)\\
		}
	}
	\textit{timer} $\leftarrow$ \textit{stopTimer}()\\
	\textit{print}(\textit{timer})\\
	$JSONGraph \leftarrow$ \textit{convertToJSON}($G$)\\
	\Return{$G$}
	\caption{\textit{NetworkX vs igraph}}
\end{algorithm}

Lo que se ha querido representar es el tiempo que tarda en construir el grafo y calcular los $k$ vecinos más cercanos \textit{(kNN)} para cada nodo. A su vez también se ha estudiado cuánta facilidad existe a la hora de convertir el grafo a formato \textit{JSON}, para que pueda ser procesado después por herramientas como \textit{d3.js}.

Los resultados obtenidos han sido los siguientes:
\\
\begin{verbatim}
	Ejecución 1:
	NetworkX: Construction and kNN time: 0.0044 seconds
	igraph: Construction and kNN time: 0.0134 seconds
	
	Ejecución 2:
	NetworkX: Construction and kNN time: 0.0020 seconds
	igraph: Construction and kNN time: 0.0111 seconds
\end{verbatim}

A su vez, en el uso del formato \textit{JSON} se encuentra más útil el uso de NetworkX ya que incluye un método propio de exportación (\textit{nx.readwrite.json\_graph}).
Por el contrario, con \textit{igraph}, habría que constuir un diccionario recorriendo los nodos y enlaces y posteriormente pasarlo a formato \textit{JSON}.

En conclusión, ante los resultados obtenidos la idea era usar \textit{NetworkX} para todas las fases del algoritmo, pero por simplicidad y facilidad en la implementación de los algoritmos, finalmente solo se utilizará para las ayudas en la visualización, y no como estructura principal de almacenamiento de los datos.

\subsubsection{Modificaciones GBILI}\label{sec5:gbili}
Con respecto al algoritmo \textit{GBILI}, surgen varias complicaciones o cuestiones de implementación que los autores no dejan claras. En este apartado se comentan las que se creen son más relevantes.
\begin{enumerate}
	\item Visualización por pasos en la web: desde un principio se tiene claro que lo ideal es tener una visualización por pasos del grafo, localizando las principales fases y mostrando como van cambiando hasta llegar a la inferencia. En este caso, el pseudocódigo \ref{alg:Gbili} muestra que hay un bucle principal en el que cada lista se va construyendo para cada nodo recorrido en este bucle externo. Para la visualización requerida esto no es lo ideal, ya que no podemos aislar las principales fases: lista de vecinos, lista de vecinos mutuos, grafo de distancias mínimas y grafo definitivo. Por todo esto, se decide seguir la misma estructura pero implementando todas estas estructuras de forma consecutiva.
	\item Durante la implementación inicial del grafo, se asumió que cualquier enlace dentro del grafo sería bidireccional, asegurando así que el grafo fuera no dirigido. Sin embargo, en una de las primeras implementaciones, esta premisa no se respetó, lo que resultó en una estructura dirigida. Este error llevó a situaciones erróneas y confusas al realizar el seguimiento del código. Posteriormente, se corrigió la implementación para garantizar que todos los enlaces sean bidireccionales, asegurando así la correcta representación de un grafo no dirigido.
	\item Dificultades de interpretación del pseucocódigo: en el código del artículo pueden surgir ciertas dudas de como hace algún paso. Estas son: al hacer la búsqueda en anchura, dice que debe retornar la componente (subgrafo aislado) del grafo completo, posteriormente, esta sintaxis la utiliza para los nodos individuales. Después de consultarlo con el tutor se llega a la conclusión de que la búsqueda en anchura realmente devuelve todo el conjunto de componentes del grafo y posteriormente se accede a la que pertenece cada nodo individual. Otra sintaxis que puede llevar a confusión es la que encontramos en la línea 20. Cuando comprueba que la distancia es mínima, la suma la hace dentro de los dos bucles, lo que puede llevar a interpretación de que esa condición se debe cumplir dentro del bucle interno. En esta situación, pongamos el siguiente ejemplo: la lista actual de vecinos mutuos es {0: [1, 2], 1: [0]...} y dos de los nodos etiquetados son [3, 4...]. Se accede a la lista de vecinos del nodo 0 y despues se recorre toda la lista de nodos etiquetados. Viendo esto sabríamos que obtendríamos una distancia mínima hasta un nodo etiquetado, pero siempre se estaría guardando el enlace entre los nodos vecinos, obteniendo la misma estructura que la lista de vecinos mutuos. Por ello, al implementarlo, hay que tener en cuenta que esta condición debe ir fuera del bucle interno, para poder coger de verdad la mínima distancia entre un nodo y sus vecinos.
	\item Visualización con NetworkX: Para tener un primer acercamiento a una visualización y también para comprobar que el algoritmo ha seguido correctamente la influencia de los nodos etiquetados, se decide mostrar los 4 pasos gráficamente. Un aspecto importante en la implementación es que con networkX, a la hora de construir el grafo, debe estar ordenado de la misma manera que los datos de entrada al algoritmo, si no mostrará información falsa, como nodos pintados como etiquetados que no lo son.
	\imagen{../img/memoria/networkxGBILI.png}{Visualización correcta de las 4 fases de construcción del grafo con el algoritmo GBILI utilizando el \textit{dataset} de \textit{iris data}. El color de la instancia representa su clase}{1}
\end{enumerate}
\subsubsection{Simplificación RGCLI}
El algoritmo original~\cite{rgcli} está preparado para casos de uso en los que el conjunto de datos de entrada es muy grande, utilizando hilos para separar estos datos de entrada y optimizar la ejecución. La implementación que se ha realizado en este caso no es concurrente, teniendo en cuenta todos los datos de entrada en las llamadas a los métodos, facilitando así la ejecución en la web y su posterior visualización.

\subsubsection{LGC adaptado a grafos}\label{sec5:LGC}
En el algoritmo original de Consistencia Local y Global, la matriz de afinidad $W$ se construye utilizando una función exponencial sobre la matriz de distancias entre los puntos. Sin embargo, dado que en este caso ya se ha construido un grafo $G$, utilizaremos esta información para definir la matriz de afinidad $W$. Específicamente, $W$ será una matriz binaria donde $W_{ij}=1$ si hay un enlace entre los nodos $i$ y $j$ en el grafo $G$, y $W_{ij}=0$ de lo contrario. Esta modificación aprovecha la estructura del grafo previamente construida, además, es la misma seguida por los autores de~\cite{gbili}. Aunque es una opción algo <<brusca>>, debe funcionar cuando los parámetros de los algoritmos son lo suficientemente buenos.

Este ajuste del algoritmo de Consistencia Local y Global permite aprovechar la estructura del grafo construido a partir de los puntos de datos, en lugar de basarse únicamente en la matriz de distancias. Esta modificación es especialmente útil cuando se tiene información topológica adicional, mejorando potencialmente la precisión en la inferencia de etiquetas para los puntos no etiquetados.

Un apunte importante es que cuando se estaba implementando esta modificación, se piensa que la matriz de afinidad se debe de construir aplicando la misma fórmula que antes, pero tomando como matriz de distancias las nuevas medidas de 0s y 1s. Esto no debe ser así y fue corregido ya que si no la matriz de afinidad ya no sería esa matriz binaria que utilizan los autores del artículo original.

Si la construcción del grafo ocasiona nodos aislados (sin ninguna conexión), esto provoca que esta matriz de afinidad binaria no tenga ningún 1 conectado a este nodo. Si lo reflejamos en el pseudocódigo \ref{alg:LGC}, en el paso 2, se requiere una división para realizar la inversa de la matriz $D$, que como indica el pseudocódigo, es una matriz diagonal que cada elemento $(i, i)$ es igual a la suma de valores de la fila de la matriz de afinidad binaria. En el caso propuesto, esta suma sería 0, lo que ocasiona una indeterminación al calcular la división correspondiente. La solución aportada a este problema es la \textbf{regularización}, consiste en sumar a cada valor de la diagonal de la matriz donde ocurren los problemas, un valor cercano a 0, como 0.0001. De esta manera, no afecta con las siguientes operaciones ya que la influencia en los resultados es mínima, y no se producen indeterminaciones al no tener que dividir entre 0.

Para comprobar la buena funcionalidad del algoritmo \textit{LGC} se realizará un estudio que tiene como objetivo evaluar la influencia de dos parámetros clave como son el parámetro alpha y la tolerancia. Para esto es necesario construir previamente el grafo, por ello se usará el algoritmo GBILI y se le dará un valor de $k$ vecinos igual a 10, que es el valor que los autores de~\cite{gbili} indican con el cual a partir de él los resultados se estabilizan. La eficacia del modelo se medirá en términos de precisión (accuracy).

En una primera implementación, se quiere ver la zona de valores donde mejores resultados se obtienen. Los valores de \textit{alpha} y \textit{tolerance} en este caso tendrán un rango amplio para focalizarse después en una zona concreta. Después de tres ejecuciones con diferentes datasets, se pueden ver los resultados en la figura~\ref{fig:../img/memoria/output_LGC.png}.

\imagen{../img/memoria/output_LGC.png}{Mapa de calor que representa el \textit{accuracy} para el algoritmo \textit{LGC} con rangos de valores \textit{tolerance}=[0.1, 8] y \textit{alpha}=[0.01, 1]. Se muestran los resultados para los \textit{datasets}: \textit{iris, breast cancer y wine}.}{0.8}
Lo que se puede observar en estas representaciones es que la tolerancia debe tener un valor más pequeño para que el algoritmo pueda realizar más de una iteración. En los tres gráficos coincide que las zonas rojas (valores más altos) se encuentran en la esquina inferior izquierda, es decir, tolerancia más pequeña y un valor de \textit{alpha} más cercano a 1. Partiendo de la premisa que nos dan los autores en el artículo~\cite{LGC}, en el que indican que utilizan un valor de \textit{alpha} de 0.99 para sus ejecuciones, la evaluación de momento va por buen camino. Cuánto mayor sea el conjunto de datos de entrada, como pasa con el ejemplo de \textit{wine}, la \textit{accuracy} disminuye para estos parámetros de entrada, una hipotesis ante esto es porque la tolerancia está frenando antes de que la matriz $F$ converja a valores reales.
Para comprobar esto, se realizará otro estudio ahora con un rango de valores entre \textit{alpha}=[0.90, 0.99] y una tolerancia=[0.00001, 1]. Se puede observar en la figura \ref{fig:../img/memoria/output_LGC2.png}.
\imagen{../img/memoria/output_LGC2.png}{Mapa de calor que representa el \textit{accuracy} para el algoritmo \textit{LGC} con rangos de valores \textit{alpha}=[0.90, 0.99] y \textit{tolerance}=[0.00001, 1]. Se muestran los resultados para los \textit{datasets}: \textit{iris, breast cancer y wine}.}{0.8}

En cuanto a los dos primeros gráficos, se observa que los resultados son muy buenos a lo largo de todo el gráfico, con una \textit{accuracy} cerca o por encima del 90\% en todos los casos. Esto indica que para estos conjuntos de datos, el algoritmo es altamente efectivo y logra clasificaciones muy precisas.

En cambio, al evaluar el último gráfico, se ve que, aun disminuyendo los valores de tolerancia, la precisión no ha conseguido mejorar como en los otros dos casos. Este comportamiento sugiere que el tercer conjunto de datos presenta un desafío mayor para el algoritmo, lo que impide alcanzar altos niveles de precisión. Es importante reflexionar que algunos conjuntos de datos (en este caso el dataset \textit{wine}) son inherentemente más complejos que otros, ya sea por su distribución de clases, por sus atributos u otras características intrínsecas.

\subsubsection{Grafos de fuerza en la web}
Las simulaciones de fuerzas son una técnica comúnmente utilizada en la visualización de grafos para representar nodos y enlaces de manera que se asemejen a una disposición física natural. Estas simulaciones utilizan integradores numéricos, como el integrador de \textit{Verlet}~\cite{enwiki:verlet}, para calcular las posiciones y velocidades de los nodos a lo largo del tiempo bajo la influencia de diversas fuerzas. En particular, D3.js proporciona una implementación robusta de estas simulaciones, facilitando la creación de visualizaciones dinámicas e interactivas de grafos en la web~\cite{d3-simulation}.

La simulación de fuerzas en D3.js se basa en un paso de tiempo constante $\Delta t = 1$ y una masa constante $m = 1$ para todas las partículas (nodos). Una fuerza $F$ actuando sobre una partícula es equivalente a una aceleración constante $a$ durante el intervalo de tiempo $\Delta t$, que se simula sumando esta fuerza a la velocidad de la partícula, y posteriormente sumando esta velocidad a la posición de la partícula.

D3.js proporciona la función \texttt{d3.forceSimulation} para crear una nueva simulación con un conjunto especificado de nodos. La simulación empieza automáticamente y puede ser controlada mediante eventos \texttt{tick} que se disparan en cada iteración, permitiendo actualizar las posiciones y las visualizaciones de los nodos en cada paso.

En este trabajo, se ha utilizado una simulación de fuerzas para representar un grafo en la web y destacan los siguientes aspectos de su configuración:

\begin{itemize}
	\item Se ha decidido hacer un grafo de fuerza central. Esta fuerza modifica las posiciones de los nodos en cada aplicación; no modifica las velocidades, ya que hacerlo normalmente haría que los nodos se sobrepasaran y oscilaran alrededor del centro deseado. Esta fuerza ayuda a mantener los nodos en el centro de la ventana gráfica~\cite{fuerzaCentral}. La fuerza de atracción tiene una intensidad de $0.1$. Aumentar este valor resultaría en una atracción más fuerte hacia el centro, concentrando los nodos. Disminuirlo reduciría la atracción, permitiendo una mayor dispersión.
	\item \texttt{charge}: Añade una fuerza de repulsión entre los nodos. La fuerza de repulsión tiene una intensidad de $-100$. Un valor más negativo resultaría en una mayor repulsión entre nodos, dispersándolos más en el espacio. Un valor menos negativo (o positivo) disminuiría la repulsión, haciendo que los nodos se acerquen más entre sí. También se establece un valor de \texttt{theta(0.1)}: parámetro de precisión para el cálculo de la repulsión. Valores más bajos mejoran la precisión pero aumentan el tiempo de cálculo; valores más altos reducen la precisión y el tiempo de cálculo. Y por último un valor de \texttt{distanceMax(500)}: establece la distancia máxima a la que la fuerza de repulsión tiene efecto. Aumentar este valor incrementaría el rango de influencia de la repulsión, afectando a más nodos. Disminuirlo limitaría la repulsión a nodos más cercanos.
	\item Durante cada evento \texttt{tick}, las posiciones de los nodos (\texttt{cx}, \texttt{cy}) y los enlaces (\texttt{x1}, \texttt{y1}, \texttt{x2}, \texttt{y2}) se actualizan para reflejar los cambios calculados por la simulación. Esto permite una representación dinámica y fluida del grafo en la interfaz web, mejorando la comprensión visual de la estructura y relaciones dentro del grafo.
	
\end{itemize}

En resumen, la configuración de la simulación de fuerzas en D3.js permite controlar de manera precisa la disposición de los nodos y enlaces del grafo, ajustando las fuerzas de repulsión y atracción para obtener una representación visual óptima. En este caso se ha utilizado una configuración compacta para que el grafo se vea dentro del marco de visualización.

\subsection{Despliegue de la Aplicación Web}
En este trabajo, se ha desplegado una aplicación web utilizando diversas herramientas y servicios. A continuación, se detallan los pasos seguidos y la teoría detrás de cada uno de ellos~\cite{deployApp}.

\subsubsection{Preparación del Entorno de Despliegue}

Para desplegar la aplicación web, se ha utilizado un servidor en la nube proporcionado por \textit{Azure}, aprovechando el mes gratuito que ofrece este proveedor. El primer paso consistió en crear una cuenta en \textit{Azure} y configurar una máquina virtual (VM) adecuada para ejecutar la aplicación.

\begin{itemize}
	\item \textbf{Creación de la VM en \textit{Azure}}: Se selecciona una imagen de sistema operativo (Ubuntu en mi caso) y se configuran los parámetros de red, almacenamiento y seguridad. \textit{Azure} proporciona una interfaz gráfica para facilitar este proceso.
\end{itemize}

\subsubsection{Configuración del Servidor Web}

Para manejar las solicitudes HTTP, se ha utilizado Nginx como servidor web. \textit{Nginx} es conocido por su alto rendimiento y eficiencia en la gestión de conexiones concurrentes. La configuración de \textit{Nginx} se realiza mediante un archivo de configuración, donde se definen aspectos como el servidor que escucha en el puerto 80 y el \textit{proxy} hacia \textit{gunicorn}.

\begin{itemize}
	\item \textbf{Instalación de \textit{Nginx}}: Se instala Nginx en la VM mediante el gestor de paquetes correspondiente.
	\item \textbf{Configuración de Nginx}: Se crea y edita un archivo de configuración ubicado en \texttt{/etc/nginx/sites-available/} y se crea un enlace simbólico en \texttt{/etc/nginx/sites-enabled/}. Este archivo contiene las directivas para redirigir las solicitudes al servicio de \textit{gunicorn} (figura~\ref{fig:../img/memoria/nginxFile.png}).
\end{itemize}

\imagen{../img/memoria/nginxFile.png}{Parte de configuración del archivo \texttt{/etc/nginx/sites-enabled/default}}{1}

\subsubsection{Implementación del Servidor de Aplicaciones}

\textit{Gunicorn} se utiliza como servidor de aplicaciones \textit{WSGI} para ejecutar la aplicación web desarrollada en Python. \textit{Gunicorn} es eficiente y compatible con varios \textit{frameworks} web de Python.

\begin{itemize}
	\item \textbf{Instalación de gunicorn}: Se instala gunicorn en la VM utilizando \texttt{pip}.
	\item \textbf{Creación del servicio de gunicorn}: Se configura un servicio de \textit{systemd} para \textit{gunicorn}, permitiendo su gestión como un servicio del sistema. El archivo de configuración del servicio se guarda en \texttt{/etc/systemd/system/app-flask.service} (figura~\ref{fig:../img/memoria/app-flask.png}).
\end{itemize}

\imagen{../img/memoria/app-flask.png}{Archivo de configuración del servicio \textit{app-flask}}{1}

\subsubsection{Gestión de Conexiones SSH}

Para la conexión segura a la VM y la gestión remota de los servicios, se ha utilizado \textit{MobaXterm}. Esta herramienta permite la conexión SSH y proporciona una interfaz gráfica para la administración de archivos y ejecución de comandos.

\begin{itemize}
	\item \textbf{Configuración de MobaXterm}: Se configura una nueva sesión SSH utilizando la dirección IP de la VM y las credenciales correspondientes.
\end{itemize}

\subsubsection{Adquisición de Dominio}

Finalmente, se adquirió un dominio mediante \textbf{Namecheap} para asociarlo con la aplicación web. Este paso incluye la configuración de los registros DNS para que apunten a la dirección IP de la VM.

\begin{itemize}
	\item \textbf{Compra del dominio}: Se adquiere un dominio a través de Namecheap.
	\item \textbf{Configuración de DNS}: Se configuran los registros A en Namecheap para que apunten a la IP de la VM, permitiendo que el dominio redirija correctamente las solicitudes.
\end{itemize}

\subsubsection{Obtención de Certificado de Seguridad}

Para asegurar la comunicación entre el cliente y el servidor, se ha obtenido un certificado de seguridad SSL utilizando \textbf{\textit{Certbot}}. \textit{Certbot} es una herramienta gratuita y automatizada para obtener y renovar certificados SSL de \textit{Let's Encrypt}.

\begin{itemize}
	\item \textbf{Instalación de \textit{Certbot}}: Se instala en la VM utilizando el gestor de paquetes correspondiente.
	\item \textbf{Obtención del certificado}: Se ejecuta con el comando adecuado para obtener el certificado SSL para el dominio adquirido. \textit{Certbot} configura automáticamente Nginx para utilizar el certificado.
\end{itemize}


\subsubsection{Flujo de Trabajo Completo}

Teóricamente, el flujo de trabajo completo involucra los siguientes pasos:

\begin{enumerate}
	\item El usuario realiza una solicitud HTTPS utilizando el dominio adquirido.
	\item Nginx recibe la solicitud y actúa como \textit{proxy}, redirigiendo la solicitud al puerto donde \textit{Gunicorn} está escuchando.
	\item \textit{Gunicorn} procesa la solicitud mediante la aplicación web Python y genera una respuesta.
	\item Nginx recibe la respuesta de Gunicorn y la envía de vuelta al usuario, asegurando la comunicación con el certificado SSL.
\end{enumerate}

Este flujo asegura que la aplicación web sea accesible y manejable de manera eficiente, aprovechando las capacidades de cada componente del sistema.

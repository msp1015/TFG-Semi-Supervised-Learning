\capitulo{5}{Aspectos relevantes del desarrollo del proyecto}
En la sección que sigue, se abordan los aspectos más significativos que han marcado el desarrollo de este proyecto. Se detalla cómo cada elección ha influido en la trayectoria y los resultados del proyecto.
\section{Lectura artículos científicos}
Se ha decidido incluir la lectura de artículos científicos como aspectos relevantes ya que son una parte significativa de todo el trabajo, llevando bastante tiempo en aquellos que son más técnicos. Además proporcionan una base sólida de teorías y métodos que pueden ser utilizadas tanto para el trabajo como para otras situaciones. Todos ellos están en inglés por lo que también se mejora esta habilidad, notando cierta mejora de comprensión en los últimos artículos.
Un artículo científico es un informe escrito y publicado en una revista con cierto prestigio que describe resultados originales de investigación. Estos artículos son fundamentales para la difusión del conocimiento científico y suelen seguir un formato estructurado que incluye una introducción, metodología, resultados, discusión y conclusiones.

Para este trabajo se han leido 2 tipos de artículos, los \textit{surveys} o resúmenes sobre un tema y los artículos de implementación de algoritmos:
\begin{itemize}
	\item \textbf{\textit{A Survey on Semi-Supervised Learning}}~\cite{Engelen:semi-supervised}: Este artículo proporciona una revisión exhaustiva del campo del aprendizaje semi-supervisado. Gracias a este artículo se consigue una base sólida en este tipo de aprendizaje y se comprenden cuales son los desafios actuales y las direcciones futuras en la investigación en este campo.
	\item \textbf{\textit{Ensemble Based Systems in Decision Making}}~\cite{ensembles}: Este artículo revisa el uso de sistemas basados en \textit{ensembles} para la toma de decisiones. Estos sistemas combinan múltiples clasificadores para mejorar la precisión y la robustez del proceso de decisión. Destaca cómo los sistemas de ensamblaje pueden superar las limitaciones de los clasificadores individuales y proporcionar decisiones más confiables y precisas.
	\item \textbf{\textit{Improve Computer-Aided Diagnosis With Machine Learning Techniques Using Undiagnosed Samples}}~\cite{IEEE:CoForest}: Los autores presentan el método Co-Forest, una técnica semi-supervisada que combina múltiples clasificadores de árboles de decisión para mejorar la precisión del diagnóstico. Se discuten los beneficios de incorporar datos no etiquetados en el proceso de entrenamiento y se presentan resultados experimentales que demuestran la eficacia de esta metodología en varias aplicaciones médicas.
	\item \textbf{\textit{Graph Construction Based on Labeled Instances for Semi-supervised Learning}}~\cite{gbili}: Los autores presentan un método para la construcción de grafos basado en instancias etiquetadas para el aprendizaje semi-supervisado. La idea principal es utilizar la información de las instancias etiquetadas para guiar la construcción del grafo.
	\item \textbf{\textit{Learning with Local and Global Consistency}}~\cite{LGC}: Este artículo introduce un algoritmo para el aprendizaje con consistencia local y global. Los autores describen cómo el algoritmo itera para ajustar las etiquetas de las instancias no etiquetadas, combinando la información de las instancias vecinas y la información inicial. Se demuestra que este enfoque es eficaz para tareas de clasificación semi-supervisada.
\end{itemize}
\section{Trabajo Preexistente}
La elección de este trabajo se realiza por el gran interés en la inteligencia artificial y el aprendizaje automático, pero también por el hecho de que ya existía un trabajo realizado por otro alumno un año atrás (David Martínez Acha -- \url{https://vass.dmacha.dev/}). Esto ayudaría mucho en el desarrollo ya que serviría como referencia para muchas dudas.

El plan original era hacer mi propia página web educativa desde cero, pero mostrando otra serie de algoritmos semisupervisados (ensembles y grafos) en lugar de los ya existentes. Con el desarrollo del primer algoritmo surge la idea por parte del tutor de basar el proyecto en esta otra página desarrollada por David Martínez. De esta manera, el tiempo que hubiera empleado en aprender e implementar la web desde cero, se emplea en comprender todo el código programado por David, aprovechando también la gestión de cuentas de usuarios. Aún así, se deja total libertad para cambiar e implementar lo que haga falta para mejorar el proyecto original.

El tiempo empleado en ajustarse al nuevo código fue de dos sprints, ya que no solo trataba de leer código, sino de comprender las técnicas de HTML, css y javascript que se utilizan, junto con bibliotecas como \textit{Bootstrap}.
Aún así, el tiempo ganado es considerable y da pie a poder implementar más algoritmos y pensar en ideas que mejoran la web.
Inicialmente se piensa que con un fork a su repositorio de GitHub,~\cite{GH:VASS}, se puede trabajar mejor, pero de esta manera se perderían las tareas y commits hechos hasta la fecha en el repositorio de este proyecto. Por esto se decide descargar el contenido y copiarlo a el proyecto ya en desarrollo.

La documentación de David~\cite{TFG:David}, sirve de gran ayuda y también se heredan partes de ella, como los trabajos relacionados y conceptos teóricos. También se tiene en cuenta las líneas de trabajo futuras desarrolladas para poder implementarlas en este trabajo.
\subsubsection{Cambios en la web}
Gran parte de la web se reutiliza, pero a medida que se desarrolla el proyecto surgen nuevas ideas que modifican parte del comportamiento de la web que ya existía. Esta sección servirá para remarcar esas pequeñas o grandes modificaciones que sufre el diseño inicial y cual es la idea de esta nueva funcionalidad. 
La primera idea de cambio que surge es la de modificar la funcionalidad de configuración de un algoritmo. Visualizando páginas web, se da con una página que muestra algoritmos de aprendizaje automático~ \cite{web:ml-visualizer}, pero no de la misma manera que David. En esta página se permite configurar y ver resultados y estadísticas en una única ventana, con la gran diferencia de que no muetra el paso a paso en cada algoritmo, sino directamente el resultado final de la clasificación.
El primer prototipo se hace pensando en esta idea,quedando algo parecido a la figura \ref{fig:../img/memoria/prototipo_coforest.png}:
\imagen{../img/memoria/prototipo_coforest.png}{Prototipo de visualizacion de algoritmo Co-Forest}{1}
Posteriormente, gracias al tutor nos damos cuenta de que si la idea es mostrar la ejecución paso a paso, que los parámetros de configuración estén en la misma pantalla, no es de gran ayuda. Por esto, se decide volver a la versión del trabajo base y modificar las plantillas necesarias para adaptarlas a los nuevos algoritmos.

\textbf{Configuración de parámetros:} Cuando tratamos con archivos de datos preparados para estos algoritmos, siempre se suele dar el caso de que la clase o etiqueta suele ser la última columna. Por esto, se ha mantenido la opción de seleccionar el atributo deseado, pero saldrá por defecto la clase en la caja de entrada. Esto además permite mayor agilidad a la hora de hacer pruebas de visualización ya que no hay que gastar tiempo en seleccionar esta opción.

\textbf{Gráfica de estadísticas específicas:} el cambio que se ha realizado en esta parte ha sido la opción de marcar o desmarcar todos los clasificadores para ver su traza en la gráfica de estadísticas. Sirve sobretodo en el caso del Co-Forest por el hecho de que puede haber gran cantidad de árboles, y querer ver uno concreto con la configuración anterior llevaba una perdida de tiempo innecesaria desmarcando uno por uno el resto de clasificadores.

\textbf{Utilizar fichero por defecto}: en la versión anterior se dedica una ventana entera a la selección del archivo, permitiendo descargar varios ficheros de prueba para luego subirlos. La idea aquí es agilizar el proceso de utilizar estos ficheros de prueba, aunque manteniendo la opción de descarga, permitiendo que al pulsar en un dataset de prueba pase a la fase de configuración directamente, sin tener que cargarlo desde las descargas.
\section{Algoritmos}
En esta sección se comentarán los aspectos más relevantes en la implementación de los algoritmos semisupervisados.
\subsection{Co-Forest}\label{sec5:coforest}
En la implementación de este algoritmo de nuevo se parte con una gran ventaja. El año anterior otra alumna había implementado el mismo algoritmo para otro proyecto. Dentro de esta aplicación, Patricia Hernando, hizo sus propios estudios, los cuales se han aprovechado en este trabajo.
Basado fuertemente en el pseudocodigo del artículo \cite{IEEE:CoForest}, la implementación se ve alterada por el parámetro W, el cual establece la confianza en las muestras para ser seleccionada o no. Resumiendo el estudio de Patricia, como se puede ver en el pseudocodigo del articulo, hay una ecuación donde el valor de W esta dividiendo. Esto es un problema ya que según establece el algoritmo puede llegar a valer cero, provocando así una indeterminación. Uno de los estudios de Patricia determina que una de las mejores soluciones a esto es iniciar el parámetro W al minimo entre 100 y el 10\% de la cantidad de muestras etiquetadas que hay. Como se puede ver en el pseudocodigo de la sección tres, esto se aprovecha, evitando así posibles problemas.
Para determinar si el algoritmo definitivo es bueno, se compara con el de Patricia, evaluando como varía el valor de \textit{accuracy} en cada iteración del algoritmo. Los resultados se muestran en la figura \ref{fig:../img/memoria/ComparacionCoForest.png}
\imagenflotante{../img/memoria/ComparacionCoForest.png}{Comparación algoritmo CoForest utilizando diversos datasets}{1}

\subsection{Grafos}
En este apartado se comentarán todos los aspectos relevantes relacionados con la implementación de los algoritmos basados en grafos. Desde sus posibles interpretaciones y modificaciones con respecto al código original, a los diferentes estudios de comparación.

\subsubsection{Comparativa de bibliotecas de grafos}
Cuando se quiere implementar un algoritmo basado en grafos, lo ideal es utilizar una biblioteca que ayude a automatizar y mejorar el código. En \textit{Python}, existen varias bibliotecas que ayudan en esta tarea, tres de ellas son: \textit{\textbf{NetworkX}}, \textit{\textbf{igraph}} y \textit{\textbf{graph-tool}}. En esta sección se resumirá el estudio realizado para elegir la opción que mejor se adapte a las especificaciones.

Todas ellas ayudan en la construcción de grafos, pero para empezar es necesario dejar claro para que se va a utilizar esta biblioteca. En cuanto al tamaño de los grafos, no necesariamente se necesita algo que maneje grafos muy grandes (más de 10000 nodos) de manera efectiva. La mayoría de \textit{datasets} utilizados tendrán muchas menos instancias. En cuanto a la velocidad, se busca algo que sea efectivo pero sin necesidad de buscar lo mejor o más rápido, ya que los datos de entrada no van a suponer un gran esfuerzo. También hay que tener en cuenta la integración que se llevará a cabo posteriormente en la web, posiblemente con herramientas como \textit{d3.js}.

Tras una primera búsqueda queda claro que la herramienta de \textit{graph-tool} tiene un objetivo mucho más amplio y está pensado para proyectos con grafos grandes. De hecho es una herramienta que no se instala con \textit{pip} sino que necesita otra instalación.

Por lo tanto, descartada una opción, se realizará una pequeña prueba para llegar a una conclusión. A continuación se muestra el pseudocódigo utilizado para la comparación de herramientas.

\begin{algorithm}
	\label{testGraph}
	\KwIn{Dataset de prueba $L$ \textit{(digits)}}
	\KwOut{Grafo $G$ en formato \textit{JSON}}
	\BlankLine
	\textit{timer} $\leftarrow$ \textit{startTimer}()\\
	$G \leftarrow \emptyset$\\
	$D \leftarrow$ \textit{pairwiseDistances}($L$) // Matriz de distancias\\
	\For{$i = 0$ \KwTo $|L|-1$}{
		// Agregar vértices al grafo para cada muestra de $L$\\
		$G \leftarrow$ \textit{addNode}($i$)\\
	}
	$k \leftarrow 5$\\
	\For{$i = 0$ \KwTo $|L|-1$}{
		$kNN \leftarrow$ \textit{getKNearestNeighbors}($D[i]$, $k$)\\
		\For{$j \in kNN$}{
			// Añadir arista entre $i$ y $j$ con el peso de la distancia\\
			$G \leftarrow$ \textit{addEdge}($i$, $j$, $D[i][j]$)\\
		}
	}
	\textit{timer} $\leftarrow$ \textit{stopTimer}()\\
	\textit{print}("Tiempo de construcción y kNN: ", \textit{timer})\\
	$JSONGraph \leftarrow$ \textit{convertToJSON}($G$)\\
	\Return{$G$}
	\caption{\textit{NetworkX vs igraph}}
\end{algorithm}

Lo que se ha querido representar es el tiempo que tarda en construir el grafo y calcular los $k$ vecinos más cercanos \textit{(kNN)} para cada nodo. A su vez también se ha estudiado cuánta facilidad existe a la hora de convertir el grafo a formato \textit{JSON}, para que pueda ser procesado después por herramientas como \textit{d3.js}.

Los resultados obtenidos han sido los siguientes:
\\
\begin{verbatim}
	Ejecución 1:
	NetworkX: Construction and kNN time: 0.0044 seconds
	igraph: Construction and kNN time: 0.0134 seconds
	
	Ejecución 2:
	NetworkX: Construction and kNN time: 0.0020 seconds
	igraph: Construction and kNN time: 0.0111 seconds
\end{verbatim}

A su vez, en el uso del formato \textit{JSON} se encuentra más útil el uso de NetworkX ya que incluye un método propio de exportación (\textit{nx.readwrite.json\_graph}).
Por el contrario, con \textit{igraph}, habría que constuir un diccionario recorriendo los nodos y enlaces y posteriormente pasarlo a formato \textit{JSON}.

En conclusión, ante los resultados obtenidos la idea era usar \textit{NetworkX} para todas las fases del algoritmo, pero por simplicidad y facilidad en la implementación de los algoritmos, finalmente solo se utilizará para las ayudas en la visualización, y no como estructura principal de almacenamiento de los datos.

\subsubsection{Modificaciones GBILI}\label{sec5:gbili}
Con respecto al algoritmo \textit{GBILI}, surgen varias complicaciones o cuestiones de implementación que los autores no dejan claro. En este apartado se comentan las que se creen son más relevantes.
\begin{enumerate}
	\item Visualización por pasos en la web: desde un principio se tiene claro que lo ideal es tener una visualización por pasos del grafo, localizando las principales fases y mostrando como van cambiando hasta llegar a la inferencia. En este caso, el pseudocódigo \ref{alg:Gbili} muestra que hay un bucle principal en el que cada lista se va construyendo para cada nodo recorrido en este bucle externo. Para la visualización requerida esto no es lo ideal, ya que no podemos aislar las principales fases: lista de vecinos, lista de vecinos mutuos, grafo de distancias mínimas y grafo definitivo. Por todo esto, se decide seguir la misma estructura pero implementando todas estas estructuras de forma consecutiva.
	\item Grafo no dirigido: en todo momento durante la implementación se conoce que cualquier enlace dentro del grafo es bidireccional, es decir, el grafo es no dirigido. Pero en una de las implementaciones esto no se sigue, lo que da en una estructura dirigida, que al realizar el seguimiento del código produce situaciones erróneas y confusas.
	\item Dificultades de interpretación del pseucocódigo: en el código del artículo pueden surgir ciertas dudas de como hace algún paso. Estas son: al hacer la búsqueda en anchura, dice que debe retornar la componente (subgrafo aislado) del grafo completo, posteriormente, esta sintaxis la utiliza para los nodos individuales. Después de consultarlo con el tutor se llega a la conclusión de que la búsqueda en anchura realmente devuelve todo el conjunto de componentes del grafo y posteriormente se accede a la que pertenece cada nodo individual. Otra sintaxis que puede llevar a confusión es la que encontramos en la línea 20. Cuando comprueba que la distancia es mínima, la suma la hace dentro de los dos bucles, lo que puede llevar a interpretación de que esa condición se debe cumplir dentro del bucle interno. En esta situación, pongamos el siguiente ejemplo: la lista actual de vecinos mutuos es {0: [1, 2], 1: [0]...} y dos de los nodos etiquetados son [3, 4...]. Se accede a la lista de vecinos del nodo 0 y despues se recorre toda la lista de nodos etiquetados. Viendo esto sabríamos que obtendríamos una distancia mínima hasta un nodo etiquetado, pero siempre se estaría guardando el enlace entre los nodos vecinos, obteniendo la misma estructura que la lista de vecinos mutuos. Por ello, al implementarlo, hay que tener en cuenta que esta condición debe ir fuera del bucle interno, para poder coger de verdad la mínima distancia entre un nodo y sus vecinos.
	\item Visualización con NetworkX: Para tener un primer acercamiento a una visualización y también para comprobar que el algoritmo ha seguido correctamente la influencia de los nodos etiquetados, se decide mostrar los 4 pasos gráficamente. Un aspecto importante en la implementación es que con networkX, a la hora de construir el grafo, debe estar ordenado de la misma manera que los datos de entrada al algoritmo, si no mostrará información falsa, como nodos pintados como etiquetados que no lo son.
	\imagen{../img/memoria/networkxGBILI.png}{Visualización correcta de las 4 fases de construcción del grafo con el algoritmo GBILI utilizando el \textit{dataset} de \textit{iris data}, cada color de una instancia representa una clase}{1}
\end{enumerate}
\subsubsection{LGC adaptado a grafos}\label{sec5:LGC}
En el algoritmo original de Consistencia Local y Global, la matriz de afinidad $W$ se construye utilizando una función exponencial sobre la matriz de distancias entre los puntos. Sin embargo, dado que en este caso ya se ha construido un grafo $G$, utilizaremos esta información para definir la matriz de afinidad $W$. Específicamente, $W$ será una matriz binaria donde $W_{ij}=1$ si hay un enlace entre los nodos $i$ y $j$ en el grafo $G$, y $W_{ij}=0$ de lo contrario. Esta modificación aprovecha la estructura del grafo previamente construida, además, es la misma seguida por loa autores de~\cite{gbili}. Aunque es una opción algo <<brusca>>, debe funcionar cuando los parámetros de los algoritmos son lo suficientemente buenos.

Este ajuste del algoritmo de Consistencia Local y Global permite aprovechar la estructura del grafo construido a partir de los puntos de datos, en lugar de basarse únicamente en la matriz de distancias. Esta modificación es especialmente útil cuando se tiene información topológica adicional, mejorando potencialmente la precisión en la inferencia de etiquetas para los puntos no etiquetados.

Un apunte importante es que cuando se estaba implementando esta modificación, se piensa que la matriz de afinidad se debe de construir aplicando la misma fórmula que antes, pero tomando como matriz de distancias las nuevas medidas de 0s y 1s. Esto no debe ser así y fue corregido ya que si no la matriz de afinidad ya no sería esa matriz binaria que utilizan los autores del artículo original.

Si la construcción del grafo ocasiona nodos aislados (sin ninguna conexión), esto provoca que esta matriz de afinidad binaria no tenga ningún 1 conectado a este nodo. Si lo reflejamos en el pseudocódigo \ref{alg:LGC}, en el paso 2, se requiere una división para realizar la inversa de la matriz $D$, que como indica el pseudocódigo, es una matriz diagonal que cada elemento $(i, i)$ es igual a la suma de valores de la fila de la matriz de afinidad binaria. En el caso propuesto, esta suma sería 0, lo que ocasiona una indeterminación al calcular la división correspondiente. La solución aportada a este problema es la \textbf{regularización}, consiste en sumar a cada valor numérico de la matriz donde ocurren los problemas, un valor cercano a 0, como 0.0001. De esta manera, no afecta con las siguientes operaciones ya que la influencia en los resultados es mínima, y no se producen indeterminaciones al no tener que dividir entre 0.

Para comprobar la buena funcionalidad del algoritmo \textit{LGC} se realizará un estudio que tiene como objetivo evaluar la influencia de dos parámetros clave como son el parámetro alpha y la tolerancia. Para esto es necesario construir previamente el grafo, por ello se usará el algoritmo GBILI y se le dará un valor de $k$ vecinos igual a 10, que es el valor que los autores de~\cite{gbili} indican con el cual a partir de él los resultados se estabilizan. La eficacia del modelo se medirá en términos de precisión (accuracy).

En una primera implementación, se quiere ver la zona de valores donde mejores resultados se obtienen. Los valores de \textit{alpha} y \textit{tolerance} en este caso tendrán un rango amplio para focalizarse después en una zona concreta. Después de tres ejecuciones con diferentes datasets, se pueden ver los resultados en la figura~\ref{fig:../img/memoria/output_LGC.png}.

\imagen{../img/memoria/output_LGC.png}{Mapa de calor que representa el \textit{accuracy} para el algoritmo \textit{LGC} con rangos de valores \textit{tolerance}=[0.1, 8] y \textit{alpha}=[0.01, 1]. Se muestran los resultados para los \textit{datasets}: \textit{iris, breast cancer y wine}.}{0.8}
Lo que se puede observar en estas representaciones es que la tolerancia debe tener un valor más pequeño para que el algoritmo pueda realizar más de una iteración. En los tres gráficos coincide que las zonas rojas (valores más altos) se encuentran en la esquina inferior izquierda, es decir, tolerancia más pequeña y un valor de \textit{alpha} más cercano a 1. Partiendo de la premisa que nos dan los autores en el artículo~\cite{LGC}, en el que indican que utilizan un valor de \textit{alpha} de 0.99 para sus ejecuciones, la evaluación de momento va por buen camino. Cuánto mayor sea el conjunto de datos de entrada, como pasa con el ejemplo de \textit{wine}, la \textit{accuracy} disminuye para estos parámetros de entrada, una hipotesis ante esto es porque la tolerancia está frenando antes de que la matriz $F$ converja a valores reales.
Para comprobar esto, se realizará otro estudio ahora con un rango de valores entre \textit{alpha}=[0.90, 0,99] y una tolerancia=[0.00001, 1]. Se puede observar en la figura \ref{fig:../img/memoria/output_LGC2.png}
\imagen{../img/memoria/output_LGC2.png}{Mapa de calor que representa el \textit{accuracy} para el algoritmo \textit{LGC} con rangos de valores \textit{alpha}=[0.90, 0.99] y \textit{tolerance}=[0.00001, 1]. Se muestran los resultados para los \textit{datasets}: \textit{iris, breast cancer y wine}.}{0.8}

En cuanto a los dos primeros gráficos, se observa que los resultados son muy buenos a lo largo de todo el gráfico, con una \textit{accuracy} cerca o por encima del 90\% en todos los casos. En cambio, al evaluar el último gráfico se ve que aún disminuyendo los valores de tolerancia, la precisión no ha conseguido mejorar como los otros dos. Aún así los resultados en esta última figura indican que los mejores resultados se encuentran en un \textit{alpha} cercano a 0.99, al igual que establecen los autores del artículo original.
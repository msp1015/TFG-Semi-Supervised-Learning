\capitulo{3}{Conceptos teóricos}

En esta sección se resumirán los conceptos teóricos básicos y necesarios para comprender el trabajo. Principalmente se hablará de aprendizaje automático y luego se profundizará en el aprendizaje semi-supervisado.

\section{Aprendizaje automático}
El aprendizaje automático (\textit{Machine Learning} en inglés) es el campo de la inteligencia artificial (IA) que se centra en el uso de datos y en el desarrollo de algoritmos para imitar la manera de aprender de los humanos \cite{ML:ibm}. La esencia radica en la capacidad de los sistemas informáticos para aprender de datos y realizar tareas sin intervención humana directa, si no descubriendo patrones y tendencias en los mismos. A estos sistemas se les conoce como \textbf{modelos}, los cuales pueden mejorar su rendimiento y adaptarse a nuevas situaciones basándose en la experiencia pasada.

Según \cite{ML:DataScientest}, existen cuatro etapas principales en el desarrollo de un modelo. El primer paso consiste en seleccionar y preparar el conjunto de datos (\textit{dataset}) que utilizará el modelo para aprender a resolver el problema para el que se ha diseñado. En el segundo paso se selecciona el algoritmo para ejecutar sobre el \textit{dataset}. Este dependerá del tamaño y el tipo de los datos de entrada y del tipo de problema que se está resolviendo. El tercer paso consiste en entrenar el algoritmo hasta que la mayoría de los resultados sean los esperados. El cuarto y último paso trata de usar el modelo sobre nuevos datos y hacer una evaluación para una posible mejora.

Según los datos que se seleccionen en el primer paso, podemos tener dos ramas distintas en el aprendizaje automático \cite{ML:SisInt}:
\begin{itemize}
	\item \textbf{Predictiva}: también caracterizada por utilizar el aprendizaje supervisado, es decir, datos de entrada etiquetados.
	\item \textbf{Descriptiva}: al contrario, utiliza el aprendizaje no supervisado, con datos de entrada no etiquetados.
\end{itemize}

 
\subsection{Aprendizaje supervisado}
El aprendizaje supervisado es un tipo de aprendizaje automático en el que los modelos son entrenados utilizando conjuntos de datos etiquetados, en los que se basarán las decisiones y predicciones. Los conjuntos de datos contienen ejemplos emparejados de variables de entrada (o características) y de salida (o etiquetas). La esencia de este tipo de aprendizaje se basa en la capacidad del modelo para aprender la relación funcional entre las entradas y las salidas, permitiéndole hacer predicciones precisas sobre nuevos datos no vistos \cite{SL:guide}. De ahí su clasificación como \guillemetleft predictiva\guillemetright ~en la sección anterior.
Dos tareas habituales dentro del aprendizaje supervisado son:
\begin{itemize}
	\item \textbf{Clasificación}: los modelos asignan categorías o clases a las entradas no etiquetadas. Dentro de este tipo se puede encontrar la clasificación binaria y la multi-clase. La primera se ve en un caso como la clasificación de correos electrónicos marcadas como spam o no spam (solo una etiqueta). Y la segunda se puede ver en cualquier ejemplo en el que haya mas de dos clases, como al establecer si un paciente tiene alto, medio o bajo riesgo de muerte ante una operación.
	\item \textbf{Regresión}: es similar a la clasificación, pero en vez de asignar un valor discreto, ahora es un valor continuo. Un ámbito común en el que se suele dar es en la economía, con la predicción de acciones o ventas.
\end{itemize}

También es importante comentar las principales fases que forman este aprendizaje y los posibles problemas o desafíos que pueden surgir, ya que pueden servir para tener en cuenta en los algoritmos concretos a implementar.
En la mayoría de algoritmos que utilizan datos etiquetados, estos se dividen en tres conjuntos: entrenamiento, validación y prueba. El conjunto de entrenamiento se utiliza para ajustar los parámetros del modelo, el conjunto de validación para ajustar los hiperparámetros y prevenir el sobreajuste y el conjunto de prueba apra evaluar el rendimiento final.
El sobreajuste u \textit{\textbf{overfitting}} es uno de los principales problemas del aprendizaje automático y ocurre cuando el modelo se ajusta demasiado a los datos de entrenamiento, es decir, los memoriza en vez de generalizar.

\subsection{Aprendizaje no supervisado}
El aprendizaje no supervisado hace referencia a los tipos de problemas en los que se utiliza un modelo para caracterizar o extraer relaciones en los datos.
A diferencia del aprendizaje supervisado, estos algoritmos descubren la estructura implícita de un conjunto de datos utilizando únicamente características de entrada y no clases o categorías. 
Ya que no existen etiquetas en los datos, los métodos no supervisados se utilizan normalmente para crear una representación concisa de los datos, posibilitando la generación de contenido creativo a partir de ellos. Por ejemplo, si tenemos una gran cantidad de fotografías sin clasificar, un modelo no supervisado encontraría relaciones entre las caracteristicas para poder organizar automáticamente las imágenes en grupos~\cite{USL:guide}.
Se pueden clasificar en tres diferentes tareas:
\begin{itemize}
	\item \textbf{Clustering}: segmentación o agrupamiento. Consiste en la identificación de grupos o \textit{clusters} en función de sus similitudes y diferencias. Dentro de este tipo, se puede diferenciar un agrupamiento exclusivo, donde los datos pertenencen a un unico grupo, y un agrupamiento superpuesto, donde los datos pueden perteneces a varias agrupaciones. El ejemplo de las fotografías entra dentro de esta categoría.
	\item \textbf{Reglas de asociación}: utiliza una medida de interés para obtener un conjunto de reglas sólidas que permitan descubrir asociaciones interesantes entre las características de un conjunto de datos. La principal aplicación es el \guillemetleft análisis de cestas de compra\guillemetright, que se usa para determinar los patrones de compra de los clientes en funcion de las relaciones entre productos.
	\item \textbf{Reducción de dimensionalidad}: estos algoritmos buscan reducir la complejidad de un conjunto de datos de alta dimensión a espacios de baja dimensión sin perder propiedades fundamentales de los datos originales. Este tipo de algoritmos se utiliza en la fase de análisis de datos, facilitando la representación gráfica. Se puede ver un ejemplo a continuación.
\end{itemize}

%TODO: ORGANIZAR ESTA PARTE

\imagen{../img/memoria/ML-ReduceDimension.png}{Reducción de dimensionalidad}{1} 


%https://www.wolfram.com/language/introduction-machine-learning/machine-learning-par%adigms/img/2-machine-learning-paradigms-Print-6.en.png

En la siguiente tabla se resumen las principales diferencias entre aprendizaje supervisado y no supervisado:
\begin{table}[ht]
	\centering
	\begin{tabular}{@{}p{2.5cm} p{5cm} p{5cm}@{}}
		\toprule
			 & \textbf{Supervisado} & \textbf{No supervisado} \\
		\midrule
		\textbf{Objetivo} & Aproximar una función que asigna entradas a salidas a partir de un conjunto de datos clasificados. & Crear una representación concisa de los datos, posibilitando la generación de contenido creativo a partir de ellos. \\
		\addlinespace[0.5em]
		\textbf{Complejidad} & Simple & Mayor\\
		\addlinespace[0.5em]
		\textbf{Entrada} & Se conoce el número de clases (datos etiquetados). & No se conoce el número de clases (datos no etiquetados). \\
		\addlinespace[0.5em]
		\textbf{Salida} & Genera un valor de salida esperado. & No se tienen valores de salida asociados \\
		\addlinespace[0.5em]
		\textbf{Tareas} & Clasificación, Regresión & Clustering, Reglas de asociación, Reducción de dimensionalidad \\
		\bottomrule
	\end{tabular}
	\caption{Comparación aprendizaje supervisado y no supervisado ~\cite{USL:guide}.}
	\label{supervisado_VS_noSupervisado}
\end{table}
\newpage


\section{Aprendizaje semi-supervisado}
Como el nombre sugiere, el aprendizaje semi-supervisado se encuentra entre los dos tipos vistos anteriormente. Los algoritmos dentro de esta estrategia se basan en extender cualquiera de los aprendizajes, supervisado o no supervisado, para añadir información adicional que el otro no proporciona~\cite{Intro:SemiSupervised}.

Los métodos de clasificación semi-supervisada intentan utilizar puntos de datos no etiquetados para generar un modelo cuyo rendimiento supere el de los modelos obtenidos al utilizar solo datos etiquetados ~\cite{Engelen:semi-supervised}. 

Por ejemplo, imaginemos que se esta trabajando en la clasificación de imágenes médicas para identificar diferentes tipos de enfermedades. En este caso, consideramos específicamente la detección temprana de ciertos tipos de cáncer a partir de imágenes de tomografías. En un enfoque supervisado, podríamos entrenar un modelo utilizando un conjunto de datos etiquetado que incluye imágenes con diagnósticos de cáncer y sin cáncer. Sin embargo, la obtención de un gran conjunto de datos etiquetado puede ser costosa y consume tiempo. En un escenario de aprendizaje semi-supervisado, además de los datos etiquetados, podríamos tener un conjunto de datos mucho más grande que incluye imágenes no etiquetadas. Algunas de estas pueden contener señales sutiles o características asociadas con el cáncer que no han sido previamente etiquetadas.

El modelo de aprendizaje semi-supervisado  podría analizar estas imágenes no etiquetadas y descubrir patrones que podrian indicar la presencia temprana de cáncer. Por ejemplo, podría aprender a reconocer características microscopicas especificas de las imagenes que no son evidentes para el ojo humano. Cuando se encuentra con nuevas imágenes no etiquetadas que comparten estas características, el modelo podría clasificarlas como indicativas de la presencia de cáncer, incluso si no ha visto exactamente esas características en el conjunto de datos etiquetado.
Existe una condición necesaria en el aprendizaje semi-supervisado: la distribución marginal subyacente $p(x)$ sobre el espacio de entrada debe contener información acerca de la distribución posterior $p(x|y)$ ~\cite{Engelen:semi-supervised}. Es decir, la naturaleza de los datos no etiquetados debe contener información útil para inferir las etiquetas correspondientes.
Esta suposición es básica y en la mayoría de los ejemplos se cumple. Aún asi, como la manera de interactuar entre $p(x)$ y  $p(x|y)$ no es siempre la misma, se pueden tomar malas decisiones que conllevarían un rendimiento cada vez peor. Por esta razón, existen tres principales suposiciones que todo algoritmo semi-supervisado debe cumplir para funcionar correctamente.
\begin{itemize}
	\item \textit{\textbf{Smoothness assumption}}: traducida como suposición de suavidad, consiste en que para dos puntos $x_{1}$ y $x_{2}$ que están cerca en una región densa, entonces sus correspodientes salidas (o etiquetas) $y_{1}$ y $y_{2}$ deben ser las mismas. Esto es útil sobretodo con datos no eitquetados, ya que por la propiedad transitiva, dos puntos que no estén relativamente cerca, pueden ser de la misma clase.
	\item \textit{\textbf{Low-density assumption}}: esta suposición está definida sobre la distribución de datos de entrada $p(x)$ y dice que el límite de decisión en la clasificación debe pasar antes por un área de poca densidad que por una de mayor densidad. Esto se puede observar en la figura \ref{fig:../img/memoria/Smoothness-LowDensity.png}.

	\imagen{../img/memoria/Smoothness-LowDensity.png}{\textit{Smoothness assumption} y \textit{Low-density assumption}~\cite{Engelen:semi-supervised}}{0.5}
	
	
	\item \textit{\textbf{Manifold assumption}}: esta suposición afirma que los datos utilizados se encuentran en un \textit{manifold} de baja dimensión incrustado en un espacio de mayor dimensión. En otras palabras, los datos, en lugar de proceder de cualquier parte del espacio, deben proceder de estos \textit{manifolds} de dimensiones más bajas ~\cite{web:assumptions}.
	\imagen{../img/memoria/ManifoldAssumption.png}{\textit{Manifold assumption}~\cite{web:assumptions}}{0.45}
\end{itemize}

En algunas ocasiones, aparece una cuarta suposición: \textit{\textbf{cluster assumption}}. Esta indica que dos datos que pertenecen a un mismo \textit{cluster}, pertenecen también a la misma clase. Se tomará esta suposición como una generalización de las tres anteriores ~\cite{Engelen:semi-supervised}.

\imagen{../img/memoria/ClusterAssumption.png}{\textit{Cluster assumption}~\cite{web:assumptions}}{1}

No hay una clasificación oficial de algoritmos de aprendizaje semi-supervisado, pero sí se pueden encontrar aproximaciones teniendo en cuenta las suposiciones en las que estan basadas los algoritmos y en cómo se relacionan con los algoritmos supervisados y no supervisados.
\imagen{../img/memoria/Clasificacion-SemiSupervised}{Clasificación de los diferentes algoritmos que pretenden incorporar datos no etiquetados a métodos de clasificación. Basado en ~\cite{Engelen:semi-supervised}}{1}

\subsection{Métodos inductivos}
Los métodos inductivos pretenden construir un clasificador que pueda generar predicciones para cualquier objeto del espacio de entrada. En el entrenamiento de este clasificador o modelo se pueden utilizar datos no etiquetados, pero las predicciones cuando hay varios son independientes entre sí una vez finalizado el entrenamiento.
\begin{itemize}
	\item \textbf{\textit{Wrapper methods}}: Estos métodos entrenan inicialmente clasificadores con datos etiquetados y luego utilizan las predicciones para generar datos adicionales etiquetados. Los clasificadores se vuelven a entrenar con estos datos pseudo-etiquetados.
	\item \textbf{\textit{Unsupervised preprocessing}}: Estos métodos extraen características útiles, pre-agrupan datos o determinan parámetros iniciales de aprendizaje de manera no supervisada, pero solo se aplican a datos originalmente etiquetados. Mejoran el rendimiento de clasificadores supervisados al utilizar información de datos no etiquetados durante la etapa de preprocesamiento.
	\item \textbf{\textit{Intrinsecally semi-supervised}}: Incorporan directamente datos no etiquetados en la función objetivo o procedimiento de optimización. Son extensiones de métodos supervisados al entorno semi-supervisado. Maximizan la información obtenida de datos no etiquetados durante el proceso de aprendizaje.
	
\end{itemize}
\subsection{Métodos transductivos} \label{sec3:transductivo} %TODO Reestructurar esta parte utilizando el otro articulo
A diferencia de los métodos inductivos, los métodos transductivos no construyen un modelo para todo el espacio de entrada. En su lugar, su poder predictivo se limita exactamente a los objetos que encuentra durante la fase de entrenamiento. Por lo tanto, los métodos transductivos no tienen fases de entrenamiento y predicción distintas \cite{Engelen:semi-supervised}.

El aprendizaje transductivo puede ahorrar tiempo y es preferible cuando el objetivo se orienta a mejorar nuestro conocimiento sobre el conjunto de datos sin etiquetar. Sin embargo, este enfoque tiene limitaciones, especialmente cuando queremos entender causas y efectos dentro de los datos. Básicamente, aunque el aprendizaje transductivo es útil para hacer inferencias específicas sobre los datos no etiquetados basándose en los datos etiquetados, no es adecuado ni efectivo para estudiar o predecir relaciones causales, es decir, cómo un factor directamente provoca otro. Esto se debe a que el aprendizaje transductivo se centra sólo en los datos presentes durante el entrenamiento y no generaliza más allá de estos. \cite{web:assumptions}.
Los métodos transductivos suelen definir un grafo sobre todos los puntos de datos, tanto etiquetados como no etiquetados, codificando la similitud entre pares de puntos de datos con aristas posiblemente ponderadas. Estos algoritmos se dividen en tres pasos: creación del grafo, ponderación del grafo (matriz de pesos) e inferencia (se refiere al proceso de asignar etiquetas o categorías a los nodos no etiquetados en un grafo utilizando la información de los nodos etiquetados y la estructura del grafo).
\subsubsection{Construccion de grafos}
El primer paso es la construcción de la matriz de adyacencia, que indica la presencia de aristas entre pares de nodos. Existen tres métodos posibles de construcción de grafos:
\begin{itemize}
	\item $\varepsilon$ neighbourhood: conecta cada nodo a todos los nodos a los que la distancia es como máximo $\varepsilon$. La medida de distancia normal suele ser euclídea. La estructura depende en gran medida de la elección de $\varepsilon$ y de la medida de distancia.
	
	\item $k$ - nearest neighbours: cada nodo se conecta a sus $k$ vecinos más cercanos según alguna medida de distancia. Ocurre un problema que tiene dos soluciones: symmetric $k$ nearest neighbours, que construye una arista si $i$ o $j$ estan en el “vecindario” de $k$ y mutual $k$-nearest neighbours que la construye si ambos están en la $k$ vecindad del otro.
	
	\item $b$-matching: el anterior método suele originar grafos en los que cada nodo no tiene $k$ vecinos, lo que está demostrado que afecta al rendimiento del clasificador. El objetivo del $b$-matching es encontrar el subconjunto de aristas en el grafo completo de forma que cada nodo tenga grado $b$ y la suma de los pesos de las aristas sea máxima.
\end{itemize}

El segundo paso de la construcción del grafo es ponderarlo (dar pesos a las aristas). En primer lugar, se construye una matriz de adyacencia completa utilizando una función k y después se obtiene la matriz de pesos W mediante \textit{sparsification}\footnote{Sparsification es el proceso de eliminar elementos menos significativos de una matriz o aristas en un grafo, con el fin de hacer la estructura más esparcida y eficiente en términos de almacenamiento y procesamiento.}. Uno de los métodos más populares es el de ponderación de bordes gausiano. Otro es el de linear neighbourhood propagation (LNP) que se basa en que cualquier punto de datos pueda aproximarse como una combinación lineal de sus vecinos.
El algoritmo LNP asume una estructura del grafo conocida y fija. Sin embargo, en lugar de fijar la estructura del grafo, también se puede inferir simultáneamente la estructura del grafo y los pesos de las aristas reconstruyendo linealmente los nodos basándose en todos los demás nodos.

\subsubsection{Fase de inferencia}
Existen diferentes maneras de llevar a cabo esta fase:
\begin{itemize}
	\item Asignación de etiquetas duras: grafo min-cut: se añade un único nodo fuente $v+$, conectado con peso infinito a los puntos de datos positivos y un único nodo $v-$ conectado con peso infinito a los puntos de datos negativos. Por lo tanto, determinar el corte mínimo consiste en encontrar un conjunto de aristas con un peso combinado mínimo que, cuando se eliminan, dan como resultado un grafo sin rutas desde el nodo de origen hasta el noso de destino. Los nodos conectados a $v+$ se etiquetan como positivos y los conectados a $v-$ como negativos. Este enfoque puede conducir a que casi todos los datos se etiqueten con la misma.
	\item Asignación probabilística de etiquetas: campos aleatorios de Markov: el anterior método solo produce etiquetas de clase y no probabilidades, lo que es una gran desventaja. La idea principal detrás de los CRF es calcular la probabilidad conjunta de todas las etiquetas dadas las observaciones y las relaciones entre etiquetas vecinas. Esto se hace utilizando una función de energía que mide cuán compatibles son las etiquetas y las características observadas. Luego, se utiliza una distribución exponencial para traducir la función de energía en una distribución de probabilidad.
	\item Asignación probabilística eficiente de etiquetas: campos aleatorios gaussianos: La principal mejora de los GRFs con respecto a los MRFs es que los GRFs permiten una estimación más eficiente de las probabilidades de asignación de etiquetas.
	\item\textit{ Local and global consistency}: La última propuesta no maneja bien el ruido de las etiquetas ya que las etiquetas verdaderas se fijan a los puntos de datos etiquetados. En segundo lugar, en los grafos irregulares, la influencia de los nodos con un grado alto es relativamente grande. La solución es el método LGC. El primer problema se resuelve penalizando el error cuadratico entre la etiqueta verdadera y la etiqueta estimada. El segundo problema se resuelve regularizando el termino de penalización para los puntos de datos no etiquetados mediante los grados de los nodos.
\end{itemize}


\subsection{\textit{Co-Forest}}
El co-forest es una versión semisupervisada del método de clasificación \textit{random forest}, diseñado para usar tanto datos etiquetados como no etiquetados. En este método, los árboles de decisión (que conforman el \textit{random forest}) se entrenan en un proceso iterativo utilizando subconjuntos de datos etiquetados junto con pseudo-etiquetas seleccionadas de los datos no etiquetados basadas en su alta confiabilidad. La confiabilidad de estas pseudo-etiquetas es evaluada por todos los árboles del ensemble excepto el árbol que está siendo entrenado en ese momento, denominado el conjunto concomitante. El entrenamiento continúa hasta que se alcanza un criterio de parada definido como el \textit{Out Of Bag Error}(OOBE), que mide el error de predicción del conjunto concomitante usando solo aquellos árboles que no incluyeron una muestra específica de los datos etiquetados en su entrenamiento.~\cite{IEEE:CoForest}. A continuación se muestra el pseudocódigo y una explicación más extensa:
\begin{algorithm}
	\label{alg:Co-Forest}
	\KwIn{Conjunto de datos etiquetados $L$, conjunto de datos no etiquetados $U$, número de árboles $n$, umbral de confianza $\theta$, sumatorio de confianzas inicial $W_{inicial}$ y parámetros para los árboles de decision $p$}
	\KwOut{\textit{Ensemble} de árboles entrenado $H$}
	\BlankLine
	\For{$i$ = 0, ..., $n-1$}{
		$L_{i} \leftarrow$ \textit{Bootstrap}($L$)\\
		$h_i$ = EntrenarArbol($L_{i}$, $p$)\\
		$\hat{e}_{i,t} \leftarrow 0.5$\\
		$W_{i,0} \leftarrow$ $W_{inicial}$  \\
	}
	
	$t \leftarrow 0$\\
	\While(){Algún árbol reciba pseudo-etiquetas}{
		$t \leftarrow t + 1$\\
		
		\For{$i$ = 0, ..., $n-1$}{
			$\hat{e}_{i,t} \leftarrow$ EstimateError($H_i, L$)\\
			$L'_{i,t} \leftarrow \emptyset$\\
			
			\If{$\hat{e}_{i,t} < \hat{e}_{i,t-1}$}{
				$W_{max} = \hat{e}_{i,t-1}W_{i,t-1}/\hat{e}_{i, t}$\\
				$U'_{i,t} \leftarrow$ Submuestrear($U, H_i, W_{max}$)\\
				$W_{i,t} \leftarrow 0$
				
				\ForEach{$x_j \in U'_{i,t}$}{
					
					\If{\text{Confianza}($H_i, x_j$) > $\theta$}{
						$L'_{i,t} \leftarrow L'_{i,t} \cup {x_j, H_i(x_j)}$\\
						$W_{i,t} \leftarrow W_{i,t} + \text{Confianza}(H_i, x_j)$
					}
				}
			}
		}
		\For{$i$ = 0, ..., $n-1$}{
			\If{$(e_{i,t} * W_{i,t} < e_{i, t-1} * W_{i, t-1})$}{
				$h_i$ = ReentrenarArbol($L_{i} \cup L'_{i,t}$)\\
			}
		}
	}
	\Return{$H$}
	\caption{\textit{Co-Forest}}
\end{algorithm}

\clearpage

Inicialización de Variables:
\begin{itemize}
	\item Cada árbol del \textit{ensemble}, $h_i$, se inicializa entrenándolo con una muestra \textit{bootstrap} del conjunto etiquetado $L$.
	\item Se establece un error estimado inicial, $\hat{e}_{i,t}$, a 0.5 para cada árbol.
	\item Se asigna el sumatorio de confianzas inicial a cada árbol. Basado en el estudio comentado en la sección \ref{sec5:coforest}.
\end{itemize}

Proceso iterativo:
\begin{itemize}
	\item El bucle continúa mientras al menos un árbol pueda recibir nuevas pseudo-etiquetas para entrenamiento.
	\item Se estima el error actual (\textit{OOBE}) del árbol usando el conjunto etiquetado.
	\item Se inicializa un conjunto temporal $L'_{i,t}$ para acumular nuevas pseudo-etiquetas aceptadas.
	\item Si el error estimado del árbol mejora (disminuye respecto a la iteración anterior), se procede a submuestrear el conjunto no etiquetado $U$ basándose en el peso máximo $W_{max}$, que ajusta la cantidad de datos a submuestrear en función de la mejora en el error.
	\item Cada dato submuestreado se evalúa, y si la confianza en su pseudo-etiqueta supera el umbral $\theta$, se añade al conjunto temporal $L'_{i,t}$ con su correspondiente pseudo-etiqueta, y se actualiza el sumatorio de confianzas para ese árbol.
\end{itemize}

Reentrenamiento de árboles:
Después de evaluar y potencialmente agregar nuevas pseudo-etiquetas, se decide si reentrenar el árbol. Esto se basa en una comparación del producto de error y sumatorio de confianzas actual contra el de la iteración anterior. Si el producto actual es menor, se procede a reentrenar el árbol incorporando las nuevas pseudo-etiquetas. Esto ayuda a mantener la calidad del modelo y a evitar el sobreajuste.

\subsection{\textit{ Graph-based on informativeness of
labeled instances}(GBILI)}
El algoritmo \textit{GBILI} o Construcción de Grafos Basada en la Informatividad de Instancias Etiquetadas es un método de construcción de grafos para el aprendizaje semisupervisado. Su principal característica es que se basa en la informatividad de las instancias etiquetadas para relacionar nodos dentro del grafo, pudiendo aprovechar después esta conectividad para predecir las etiquetas de los datos no etiquetados. Se utilizan los métodos de $k$-vecinos más cercanos y  $k$-vecinos más cercanos mutuos para inicializar el grafo. En el pseudocódigo \ref{alg:Gbili}, propuesto en~\cite{gbili}, se puede ver el proceso detallado.

\subsubsection{Metodología}
El algoritmo aprovecha la información de las etiquetas disponibles para priorizar las conexiones entre los vértices, especialmente aquellos que están más cerca de un punto etiquetado, convirtiendo así estos puntos etiquetados en hubs a medida que aumenta el valor de k.
Los pasos que sigue son:
\begin{enumerate}
	\item Generación de la Matriz de Distancias: Se crea una matriz de distancias $D$ usando la distancia euclidiana para determinar los k vecinos más cercanos de cada elemento.
	\item Configuración de Parámetros: Se establece el parámetro $k$ con un valor natural y se genera una lista de puntos etiquetados	$L$.
	\item Búsqueda de Vecinos Más Cercanos: Para cada vértice $v_i$, se encuentran sus k vecinos más cercanos.
	\item Determinación de Vecinos Mutuos: Se identifican los k vecinos mutuos para $v_i$.
	\item Cálculo de la Informatividad: Se calcula la suma de las distancias desde $v_i$ a cada elemento fe vecinos mutuos y desde estos elementos hasta un punto etiquetado. Se establece una conexión entre $v_i$ y $v_j$ que minimice esta suma. Este aspecto puede verse confuso y por ello se comenta en la sección \ref{sec5:gbili}
	\item Post-procesamiento del Grafo: Se conectan los componentes aislados mediante una búsqueda en anchura (\textit{BFS}) para encontrar componentes en la red. Los componentes sin puntos etiquetados se conectan con componentes vecinos que tienen puntos etiquetados, limitando el número de nuevas conexiones para evitar una red demasiado densa.
\end{enumerate}


\begin{algorithm}
	\label{alg:Gbili}
	\KwIn{Conjunto de datos etiquetados $L$, conjunto de datos no etiquetados $U$, número de vecinos más cercanos $K$}
	\KwOut{Grafo $G$}
	\BlankLine
	generar matriz de distancias $D$ entre todos los puntos de datos\\
	establecer el parámetro K\\
	\For{$i$=1; $i<|V|$; $i++$}{
		\For{$k$=1; $k<K$; $k++$}{
			\For{$j$=1; $j<|V|$; $j++$}{
				\If{$D(v_i,v_j)$ es el $kNN$}{
					$listakNN(v_i) \leftarrow v_j$
				}
			}
		}
		\For{$j$=1; $j<listakNN(v_i)$; $j++$}{
			\For{$k$=1; $k<K$; $k++$}{
				\If{$D(v_j,v_i)$ es el $kNN$}{
					$listaMutuoskNN(v_i) \leftarrow v_j$
				}
			}
		}
		\For{$j$=1; $j<listaMutuoskNN(v_i)$; $j++$}{
			\For{$l$=1; $l<L$; $l++$}{
				\If{$D(v_i,v_j) + D(v_j, v_i)$ es mínima }{
					$G \leftarrow e_{i,j}$
				}
			}
		}
	}
	$Componentes = BFS(G)$\\
	\For{$i=1$; $i<|V|$; $i++$}{
		\If{$Componentes(v_i) \notin L$}{
			\For{$k=1$;$k<listakNN(v_i)$; $k++$}{
				\If{$Componentes(v_k) \in L$}{
					$G \leftarrow e_{i,k}$
				}
			}
		}
	}
	\Return{$G$}
	\caption{\textit{GBILI}}
\end{algorithm}
\clearpage
Como se comenta en uno de los pasos, hay ciertos puntos en este algoritmo que deben ser comentados para evitar confusión en la implementación. Además, para este trabajo en concreto, el algoritmo sufre ciertas modificaciones (con la misma idea teórica) que se comentan en la sección \ref{sec5:gbili}.

\subsection{\textit{Local and Global Consistency}}
Este algoritmo aborda el problema general de aprender a partir de datos etiquetados y no etiquetados. Dado un conjunto de puntos $X$ y un conjunto de etiquetas $L$, los primeros $l$ puntos tienen etiquetas y los puntos restantes no están etiquetados. El objetivo es predecir las etiquetas de los puntos no etiquetados. Esta fuertemente basado en dos de las suposiciones comentadas previamente: Los puntos cercanos tienden a tener la misma etiqueta y los puntos en la misma estructura (como un clúster) probablemente tengan la misma etiqueta.

La idea principal es que cada punto propague iterativamente su información de etiqueta a sus vecinos hasta que se alcance un estado global estable. El pseudocódigo \ref{alg:LGC} y las aclaraciones siguientes están basados en el artículo \cite{LGC}.

\begin{itemize}
	\item Crear la matriz de afinidad: Calcula una matriz $W$ que mide la similitud entre cada par de puntos en $X$. Si dos puntos están muy cerca entre sí, su valor en la matriz $W$ será alto. Los valores en la diagonal de la matriz (que corresponden a la similitud de un punto consigo mismo) se establecen en cero.
	\item Normalizar\footnote{\textbf{Normalizar} es un proceso en el que se ajustan los valores de una matriz o conjunto de datos para que sean más comparables y manejables} la matriz de afinidad: Ajusta la matriz	$W$ para obtener una nueva matriz $S$. Esto es necesario para asegurar que los valores se propaguen correctamente en los pasos siguientes.
	\item Propagar la información de etiquetas: Comienza con una matriz inicial $F$ que contiene las etiquetas conocidas. Repite un proceso en el que cada punto actualiza su etiqueta basada en las etiquetas de sus puntos vecinos y sus etiquetas iniciales. Este proceso se repite hasta que las etiquetas dejan de cambiar significativamente.
	\item Asignar etiquetas finales: Una vez que la propagación de etiquetas ha convergido (ya no cambia mucho), asigna a cada punto la etiqueta de la clase con la que tiene mayor afinidad. Esto se hace eligiendo la etiqueta que corresponde al valor más alto en la matriz $F$ para cada punto.
\end{itemize}
\begin{algorithm}
	\caption{Local and Global Consistency}
	\label{alg:LGC}
	\KwIn{Conjunto de puntos $X = \{x_1, \dots, x_l, x_{l+1}, \dots, x_n\} \subseteq {R}^m$, conjunto de etiquetas $L = \{1, \dots, c\}$}
	\KwOut{Etiquetas predichas para los puntos no etiquetados}
	
	\BlankLine
	\textbf{Paso 1: Formar la matriz de afinidad} $W$ \\
	\For{$i=1$ \textbf{to} $n$}{
		\For{$j=1$ \textbf{to} $n$}{
			\If{$i \neq j$}{
				$W_{ij} \leftarrow \exp(-\|x_i - x_j\|^2 / 2\epsilon^2)$
			}
			\Else{
				$W_{ii} \leftarrow 0$
			}
		}
	}
	
	\BlankLine
	\textbf{Paso 2: Construir la matriz} $S = D^{-1/2} W D^{-1/2}$ \\
	$D$ es una matriz diagonal con el elemento $(i, i)$ igual a la suma de la $i$-ésima fila de $W$
	
	\BlankLine
	\textbf{Paso 3: Iterar} $F^{(t+1)} = \alpha S F^{(t)} + (1 - \alpha) Y$ \textbf{hasta la convergencia} \\
	$\alpha$ es un parámetro $\in (0, 1)$
	
	\BlankLine
	\textbf{Paso 4: Asignar etiquetas} \\
	Deja que $F^*$ denote el límite de la secuencia $\{F^{(t)}\}$ \\
	\For{$i=1$ \textbf{to} $n$}{
		$y_i \leftarrow \arg\max_{j \leq c} F^*_{ij}$
	}
	
	\Return{Etiquetas predichas para los puntos no etiquetados}
	
\end{algorithm}

Este algoritmo, como se ha podido llegar a observar, podría no usar directamente la unión física entre nodos del grafo, es decir, no utilizar el paso anterior de construcción del grafo, y aún así, funcionar correctamente, ya que dispondría de la información que da la matriz de distancias. Por ello, para el interés de este trabajo, es necesario buscar una manera de utilizar esta información, la cual se comenta en la seccion \ref{sec5:LGC}
\clearpage
\section{Ensembles}
Los sistemas basados en \textit{ensembles} se refieren al proceso de combinar las opiniones de un conjunto de modelos diferentes para tomar una decisión final. Se ha descubierto que los \textit{ensembles} pueden producir resultados más favorables en comparación con los sistemas de un solo experto en una amplia gama de aplicaciones. La creación de un \textit{ensemble} implica generar componentes individuales del sistema y luego combinar sus clasificaciones. En este contexto, se destacan dos técnicas principales: \textit{bagging} y \textit{boosting}~\cite{ensembles}.

\subsection{Bagging}
El \textit{Bagging}, o \textit{Bootstrap Aggregating}, es una técnica que mejora la estabilidad y precisión de los algoritmos de aprendizaje automático. Funciona mediante la creación de múltiples versiones de un predictor y utilizando estos para obtener un conjunto agregado. Se generan diferentes conjuntos de entrenamiento, cada uno mediante muestreo con reemplazo del conjunto original, y se entrena un modelo en cada uno de ellos. La decisión final se toma por mayoría de votos para clasificación o el promedio en regresión. Esta técnica es particularmente efectiva con modelos que tienen alta varianza.
\imagen{../img/memoria/bagging.png}{Concepto de bagging \cite{web:boostingvsbaging}}{1}

\subsection{Boosting}
El \textit{Boosting} es un enfoque que construye secuencialmente un conjunto de modelos; cada nuevo modelo se enfoca en corregir los errores cometidos por los modelos anteriores. AdaBoost, uno de los algoritmos de \textit{boosting} más conocidos, ajusta los pesos de las instancias incorrectamente clasificadas para que modelos posteriores se enfoquen más en ellas. A diferencia del \textit{bagging}, el \textit{boosting} puede aumentar el riesgo de sobreajuste si el conjunto de datos es ruidoso, pero generalmente produce modelos más precisos.
\imagen{../img/memoria/boosting.png}{Concepto de boosting \cite{web:boostingvsbaging}}{1}
